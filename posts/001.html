<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Byte-Encoding Representation &#8212; cjlovering 0.0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/style.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Beam Search" href="002.html" />
    <link rel="prev" title="cjlovering" href="../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          cjlovering</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Byte-Encoding Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="002.html">Beam Search</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="003.html">Transformer Network</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/research.html">Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profile/professional.html">Professional</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/publications.html">Publications</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Byte-Encoding Representation</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="byte-encoding-representation">
<h1>Byte-Encoding Representation<a class="headerlink" href="#byte-encoding-representation" title="Permalink to this headline">¶</a></h1>
<p>This is a subword tokenization of a vocabulary. An example of how this would work and be useful at test time from Sennrich et al. <a class="reference internal" href="#sennrich2015neural" id="id1">[1]</a> is as follows:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;low&quot;</span><span class="p">,</span> <span class="s2">&quot;lowest&quot;</span><span class="p">,</span> <span class="s2">&quot;newer&quot;</span><span class="p">,</span> <span class="s2">&quot;wider&quot;</span> <span class="p">}</span>
<span class="n">BCE</span><span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="s2">&quot;low&quot;</span><span class="p">,</span> <span class="s2">&quot;er&quot;</span>
</pre></div>
</div>
<p>This is much more valuable than a symbol for an unknown word (UNK). Using the two subword segments, it would not be surpirising if a model would be able to intepret “lower” correctly.</p>
<p>To build this representation, an iterative algorithm can be used to link together the most common segments, starting with character pairs. Below is the pseudo code provided by the original authors with a few changes. This algorithm is iterative. The author provides a more optimized implementation that batches and avoids recomputation. It starts bottom up, getting the bigram counts for every pair of symbols. These symbols will startout as characters but in later steps be common strings.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For the given merge step, get the bigram counts, for every pair of symbols (strings).</span>

<span class="sd">    For each word and its freq in the corpus, add the number of instances for each of its subword parts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
    <span class="k">return</span> <span class="n">pairs</span>
</pre></div>
</div>
<p>Next, given the bi-counts of a vocabulary, merge the vocabulary to remove repetitions of this bigram.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Merge the vocab, adding in the new pair. &quot;&quot;&quot;</span>
    <span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">bigram_pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(?&lt;!\S)&#39;</span> <span class="o">+</span> <span class="n">bigram_pattern</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;(?!\S)&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
        <span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">v_out</span>

<span class="k">def</span> <span class="nf">byte_pair_encoding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For the given number of merges, find the most common pairs of symbols. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pairs</span><span class="p">[</span><span class="n">best</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;no pair has frequency &gt; 1. Stopping</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="n">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s1">&#39;l o w &lt;/w&gt;&#39;</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
  <span class="s1">&#39;f a r t h e s t &lt;/w&gt;&#39;</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
  <span class="s1">&#39;n e w e r &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
  <span class="s1">&#39;w i d e r &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span> <span class="p">}</span>
<span class="n">byte_pair_encoding</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span>
<span class="go">    &#39;l o w &lt;/w&gt;&#39; : 5,</span>
<span class="go">    &#39;f a r t h e s t &lt;/w&gt;&#39; : 5,</span>
<span class="go">    &#39;n e w e r &lt;/w&gt;&#39;: 5,</span>
<span class="go">    &#39;w i d e r &lt;/w&gt;&#39;: 5 }</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">byte_pair_encoding</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">{&#39;l o w &lt;/w&gt;&#39;: 5, &#39;f a r t h e s t &lt;/w&gt;&#39;: 5, &#39;n e w er &lt;/w&gt;&#39;: 5, &#39;w i d er &lt;/w&gt;&#39;: 5}</span>
<span class="go">{&#39;l o w &lt;/w&gt;&#39;: 5, &#39;f a r t h e s t &lt;/w&gt;&#39;: 5, &#39;n e w er&lt;/w&gt;&#39;: 5, &#39;w i d er&lt;/w&gt;&#39;: 5}</span>
<span class="go">{&#39;lo w &lt;/w&gt;&#39;: 5, &#39;f a r t h e s t &lt;/w&gt;&#39;: 5, &#39;n e w er&lt;/w&gt;&#39;: 5, &#39;w i d er&lt;/w&gt;&#39;: 5}</span>
<span class="go">{&#39;low &lt;/w&gt;&#39;: 5, &#39;f a r t h e s t &lt;/w&gt;&#39;: 5, &#39;n e w er&lt;/w&gt;&#39;: 5, &#39;w i d er&lt;/w&gt;&#39;: 5}</span>
<span class="go">{&#39;low&lt;/w&gt;&#39;: 5, &#39;f a r t h e s t &lt;/w&gt;&#39;: 5, &#39;n e w er&lt;/w&gt;&#39;: 5, &#39;w i d er&lt;/w&gt;&#39;: 5}</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p id="bibtex-bibliography-posts/001-0"><table class="docutils citation" frame="void" id="sennrich2015neural" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. <em>arXiv preprint arXiv:1508.07909</em>, 2015. URL: <a class="reference external" href="https://arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a>.</td></tr>
</tbody>
</table>
</p>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/posts/001.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2018, Charles J. Lovering.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.7.<br/>
    </p>
  </div>
</footer>
  </body>
</html>