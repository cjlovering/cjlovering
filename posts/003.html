<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Transformer Network &#8212; cjlovering 0.0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/style.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Research" href="../profile/research.html" />
    <link rel="prev" title="Beam Search" href="002.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          cjlovering</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="001.html">Byte-Encoding Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="002.html">Beam Search</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Transformer Network</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/research.html">Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profile/professional.html">Professional</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/publications.html">Publications</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Transformer Network</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#encoder-decoder">Encoder Decoder</a></li>
<li><a class="reference internal" href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
<li><a class="reference internal" href="#self-attention">Self Attention</a></li>
<li><a class="reference internal" href="#multi-head-attention">Multi Head Attention</a></li>
<li><a class="reference internal" href="#input-representation">Input Representation</a></li>
<li><a class="reference internal" href="#position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</a></li>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
<li><a class="reference internal" href="#decoding">Decoding</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="transformer-network">
<h1>Transformer Network<a class="headerlink" href="#transformer-network" title="Permalink to this headline">¶</a></h1>
<p>Most of the paper was clear. However, it was difficult to see how it would work across sequences of different length. I don’t mean how or why the positional encoding would work - this is intuitive, but rather how they construct the matrices to fit together :).</p>
<p>This notebook focuses on the unique modules the authors present, and how the system fits together. Other dependencies of this post:</p>
<ul class="simple">
<li><a class="reference internal" href="001.html"><span class="doc">Byte-Encoding</span></a></li>
<li>Layer Normalization</li>
<li>Residual Connections</li>
<li><a class="reference internal" href="002.html"><span class="doc">Beam Search</span></a></li>
</ul>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This work focuses on the task of natural language translation (e.g. english to german or vice versa.)</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">I modify implementations from the <a class="reference external" href="https://github.com/tensorflow/tensor2tensor">tensor2tensor</a> library and <a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> post, focusing on my interpretation of the paper. For a complete view and implementation of this system, please visit the mentioned sources above. All diagrams were created by me. Some are re-intepretations from the orginal paper.</p>
</div>
</div>
<div class="section" id="encoder-decoder">
<h2>Encoder Decoder<a class="headerlink" href="#encoder-decoder" title="Permalink to this headline">¶</a></h2>
<p>The transformer uses a <a class="reference external" href="https://arxiv.org/abs/1409.0473">encoder-decoder</a> structure: an input sequence of symbols, <span class="math notranslate nohighlight">\(x = { x_1, x_2, \dots, x_n }\)</span>, is encoded into a sequence of continuous variables,  <span class="math notranslate nohighlight">\(\mathbf{z} = { z_1, z_2, \dots, z_n }\)</span>. This is then decoded into a sequence of symbols, <span class="math notranslate nohighlight">\(y = { y_1, y_2, \dots, y_n }\)</span>. In some cases, <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is a single continuous variable. This generation occurs one at a time - it is <a class="reference external" href="https://arxiv.org/abs/1308.0850">auto-regressive</a>, further consuming the previously generated symbols as additional input when generating the next. Encoder and decoder models usually use a recurrent architecture.</p>
<p>According to Cho. 14, the encoding function <span class="math notranslate nohighlight">\(e\)</span> can be any non-linear function, but it is often implemented as an RNN.</p>
<div class="math notranslate nohighlight">
\[h_{&lt;t&gt;} = e(h_{&lt;t-1&gt;}, x_t)\]</div>
<div class="figure align-center" id="id2">
<img alt="../_images/encoder.svg" height="250px" src="../_images/encoder.svg" /><p class="caption"><span class="caption-text">Encoder architecture.</span></p>
</div>
<p>The input sentence <span class="math notranslate nohighlight">\(x\)</span> is encoded into the vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. Depending on the implemention, we consider the final hidden states as the encoding, or some operation on all the hidden states.</p>
<p>Next, we decode <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into the output predictions <span class="math notranslate nohighlight">\(y\)</span>. Again, this uses a recurrent function (RNN).</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_{&lt;t&gt;} = d(h_{&lt;t-1&gt;}, y_{&lt;t-1&gt;}, z)\\
y_{&lt;t&gt;} = g(h_{&lt;t&gt;}, y_{&lt;t-1&gt;}, z)\end{split}\]</div>
<div class="figure align-center" id="id3">
<img alt="../_images/decoder.svg" height="350px" src="../_images/decoder.svg" /><p class="caption"><span class="caption-text">Encoder architecture.</span></p>
</div>
<p>This structure has some issues.</p>
<ol class="arabic simple">
<li>Its sequential and for a given <span class="math notranslate nohighlight">\(x\)</span>, it cannot be parallelized.</li>
<li>Often <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is input into each decoding function, and since <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is O(n) distance to each input symbol, it becomes difficult to learn long range dependencies.</li>
<li>The path between an output symbol and its corresponding source symbol depends on the length of <span class="math notranslate nohighlight">\(x\)</span>.</li>
</ol>
<p>The transformer network uses a stateless auto-regressive strategy which will decode the encoded (but not summarized) source words and the current output words. This will allow the model to be highly parallelized.</p>
<p>The primary featured used is scaled dot product attention.</p>
</div>
<div class="section" id="scaled-dot-product-attention">
<h2>Scaled Dot-Product Attention<a class="headerlink" href="#scaled-dot-product-attention" title="Permalink to this headline">¶</a></h2>
<p>The authors describe attention as follows:</p>
<blockquote>
<div>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</div></blockquote>
<p>In addition to attention, they use a few techniques to regularize their network: layer normalization, residual connections, and dropout.</p>
<p>As noted by the authors, attention maps a query to a combination of given outputs, as determined by the query’s corresponding compatibility with the input keys. As the autological “Scaled Dot-Product Attention” method implies, the authors use dot product for their compatibility function. One could use any metric, learned or otherwise. Cosine distance or a layer of MLP can be used.</p>
<div class="figure align-center" id="id4">
<img alt="../_images/attention-explained.svg" height="350px" src="../_images/attention-explained.svg" /><p class="caption"><span class="caption-text">Attention inuition.</span></p>
</div>
<ol class="arabic simple">
<li>Each key <span class="math notranslate nohighlight">\(K_i\)</span> maps to a value <span class="math notranslate nohighlight">\(V_i\)</span>.</li>
<li>Each query <span class="math notranslate nohighlight">\(Q_j\)</span> will operate on all the keys with a compatibility function (dot product). As shown in (b), the closer the vectors are in high-dimensional space, the more compatible. These scores will be transformed into a probability distribution by a softmax.</li>
<li>Then, each query will be mapped to a linear combination of the values as determined by the probability distribution (c).</li>
</ol>
<p>As shown in the intuitive example above, the query <span class="math notranslate nohighlight">\(q_1\)</span> is most similar to <span class="math notranslate nohighlight">\(k_1\)</span>, thus it is mapped predimately to the corresponding value <span class="math notranslate nohighlight">\(v_1\)</span>. Note: these values are examples, not necessarily accurate.</p>
<p>The scaled dot product attention is straight forward.</p>
<div class="math notranslate nohighlight">
\[\begin{split}A: Q \times K \times V \to O \\
Q\in \mathbb{R}^{q \times d}, K \in \mathbb{R}^{n \times d}, V \in \mathbb{R}^{n \times v}, O \in \mathbb{R}^{q \times v} \\
A = \text{SOFTMAX}(\frac{QK^{\intercal}}{\sqrt{d}}) V\end{split}\]</div>
<p>The innovative aspect of scaled dot product - beyond the authors’ apt framing of the problem as an interaction between queries, keys, and values - is the scaling. The author motivate this scaling by noting that the variance of a dot product scales with the size of the input vectors. Increased variance will result in increased magnitude, “pushing the softmax function into regions where it has extremely small gradients.”</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Why is the gradient small?</p>
</div>
<div class="figure align-center" id="id5">
<img alt="../_images/scaled-dot-product.svg" height="350px" src="../_images/scaled-dot-product.svg" /><p class="caption"><span class="caption-text">Scaled dot product attention.</span></p>
</div>
<p>Below is an implementation for the scaled dot product. Each line corresponds to a box in the figure above.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="s2">&quot;Compute &#39;Scaled Dot Product Attention&#39;&quot;</span>
    <span class="c1"># Compatiblity function (dot product) between the query and keys.</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Scale the scores depending on the size of the inputs.</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Optional mask. This is used to zero out values that should not be used by this function.</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="c1"># Compute probability distribution across the final dimension.</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Output linear combinations of values, as determined by the distribution.</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
</pre></div>
</div>
</div>
<div class="section" id="self-attention">
<h2>Self Attention<a class="headerlink" href="#self-attention" title="Permalink to this headline">¶</a></h2>
<p>With a single query, self attention will have no effect. This is because the attention mechanism will be a linear combination of the values, and it can only reproduce itself so it serves as an identity function.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SelfAttention</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span>
    <span class="k">return</span> <span class="n">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1000</span><span class="p">,</span> <span class="mf">0.1000</span><span class="p">,</span> <span class="mf">0.8000</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">]])</span>
</pre></div>
</div>
<p>When there are multiple queries, the vectors that are most <em>compatible</em> will become even more similar as they will be mapped to combinations consisting mostly of the already compatible vectors.</p>
<p>The remaining vector will also be normalized <em>different</em>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span>
<span class="go">    [0,0,1],</span>
<span class="go">    [0,0,2],</span>
<span class="go">    [1,0,0]</span>
<span class="go">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
<span class="go">tensor([[0.2992, 0.5329, 0.1679],</span>
<span class="go">      [0.2228, 0.7070, 0.0702],</span>
<span class="go">      [0.2645, 0.2645, 0.4711]])</span>
</pre></div>
</div>
<p>Note that, especially with values greater than 1, a vector can have a greater dot product with other vectors rather than itself. So, similarity is aptly not the correct word to describe this interaction (at least when using a dot product). Thus, the first vector is mapped to a construction consisting mostly of itself and the second vector follows the same trend but more extreme. Lastly, the third vector, less compatible than the others - becomes pseduo-normalized.</p>
</div>
<div class="section" id="multi-head-attention">
<h2>Multi Head Attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this headline">¶</a></h2>
<p>The transformer uses “Multi-Head Attention” as its primary module. It is built up using scaled dot product attention.</p>
<p>Rather than attend raw queries a single time, this method attends <em>h</em> linear projections of the input. For each of the <em>h</em> heads, the inputs (K,Q,V) are projected linearily with a learned mapping. This is great! Rather than using a single dot product, the multi-headed attention can learn to project vectors and attend them differently.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ultimately, the compatiblity function and the projections are all linear - perhaps it would be worth the time to see if non-linear mappings would drastically effect the performance of this method. Does using a feed forward layer here help? hurt?</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\text{out} = \texttt{Concat}(\text{head}_0, \dots, \text{head}_h) W^O \\
\text{head}_i = \texttt{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
Q \in \mathbb{R}^{q \times m}, K \in \mathbb{R}^{n \times m}, V \in \mathbb{R}^{n \times m} \\
W_j^Q, W_j^K, W_j^V \in \mathbb{R}^{m \times d} \\
W^O \in \mathbb{R}^{(h*v)\times m}\end{split}\]</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="s2">&quot;Take in model size and number of heads.&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># We assume d_v always equals d_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="s2">&quot;Implements Figure 2&quot;</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Same mask applied to all h heads.</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> \
            <span class="p">[</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>

        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span>
        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
                                <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># 3) &quot;Concat&quot; using a view and apply a final linear.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> \
            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Thus, the multi-headed attention is a function from <span class="math notranslate nohighlight">\(R^{q \times d}\)</span> to <span class="math notranslate nohighlight">\(R^{q\ times v}\)</span>. Furthermore, like the scaled-dot-product attenion, it is able to concurrently operate on all the queries in parallel regardless of the size of the sentence.</p>
<p>Additionally, this module is able to support <em>h</em> different heads, and still output a fixed-size vector for each query by concat and then matrix multiply to reduce the dimensionality.</p>
<div class="figure align-center" id="id6">
<img alt="../_images/multi-head.svg" height="350px" src="../_images/multi-head.svg" /><p class="caption"><span class="caption-text">Scaled dot product attention.</span></p>
</div>
</div>
<div class="section" id="input-representation">
<h2>Input Representation<a class="headerlink" href="#input-representation" title="Permalink to this headline">¶</a></h2>
<p>This work used a Byte Pair Encoding scheme. This is a subword tokenization of your vocabulary. This is much more valuable than a UNK symbol. To build this representation, an iterative algorithm can be used to link together the most common segments, starting with character pairs.</p>
</div>
<div class="section" id="position-wise-feed-forward-networks">
<h2>Position-wise Feed-Forward Networks<a class="headerlink" href="#position-wise-feed-forward-networks" title="Permalink to this headline">¶</a></h2>
<p>This two linear transforms with a nonlinear (RELU) operation. The denotation of position-wise remarks on the fact that it is not a convolution, nor does it have any directly spatial functionality.</p>
<div class="math notranslate nohighlight">
\[\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\]</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;Implements FFN equation.&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
<p>The remaining features used by the network is residual layers, layer normalization and positional encoding. The structure and features of the model all work to make short paths between inputs and outputs, while also being highly regularized. Layer normalization and residual layers are topics on-to-themselves.</p>
<p>The positional encoding is used to represent the position of the queries in their embeddings. This is important because the attention mechanisms have no notion of order among the queries, and order determines the semantics of a sentence.</p>
<p>The authors use a positional encoding that uses</p>
<div class="math notranslate nohighlight">
\[\text{PE}_{(pos,2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})
\text{PE}_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\]</div>
<p>where <span class="math notranslate nohighlight">\(pos\)</span> is the position and <span class="math notranslate nohighlight">\(i\)</span> is the dimension.</p>
<p>As the authors describe:</p>
<blockquote>
<div>That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from <span class="math notranslate nohighlight">\(2\pi\)</span> to <span class="math notranslate nohighlight">\(10000 \cdot 2\pi\)</span>.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math notranslate nohighlight">\(PE_{pos}\)</span>.</div></blockquote>
<div class="admonition attention">
<p class="first admonition-title">Attention</p>
<p class="last">How can this be represented as a linear function?</p>
</div>
<div class="figure align-center" id="id7">
<img alt="../_images/pe.png" src="../_images/pe.png" />
<p class="caption"><span class="caption-text">Each dimension corresponds to its location; in the diagram below, each vertical slice of the grapch would be added to the input word embeddings.</span></p>
</div>
<div class="figure align-center" id="id8">
<img alt="../_images/pe-dropout.png" src="../_images/pe-dropout.png" />
<p class="caption"><span class="caption-text">The authors use dropout to reduce the strength of the signal; the figure demonstrates its effect.</span></p>
</div>
</div>
<div class="section" id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h2>
<p>Each instance of the transformer will output a probability for the next symbol. As you can see, the encoder and decoder stacks are repeated N times each. In the paper the default was N = 6. The input and ouput of each stack is the of the same dimensionality.</p>
<div class="figure align-center">
<img alt="../_images/architecture.svg" height="650px" src="../_images/architecture.svg" /></div>
</div>
<div class="section" id="decoding">
<h2>Decoding<a class="headerlink" href="#decoding" title="Permalink to this headline">¶</a></h2>
<p>The linear layer takes an input of <span class="math notranslate nohighlight">\(\mathbb{R}^{q \times m}\)</span> and has a weight shape of <span class="math notranslate nohighlight">\(\mathbb{R}^{m \times vocab}\)</span>, outputing <span class="math notranslate nohighlight">\(\mathbb{R}^{q \times vocab}\)</span>. During training, the decoding is set so that all subsequent positions are masked out during attention, so that a symbol could never see “into the future”. So, the final linear layer will output a probability distribution for each query (each symbol generated so far) starting with the start symbol. When decoding the next symbol will always be the right-most dimension.</p>
<p>When decoding an output sequence, the network is run repeatedly. A greedy approach looks something like this:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">greedy_decode</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">start_symbol</span><span class="p">):</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">start_symbol</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># generate a word up to the max length. the system could represent stop symbols to stop early.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
            <span class="n">memory</span><span class="p">,</span>
            <span class="n">src_mask</span><span class="p">,</span>
            <span class="n">Variable</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span>
            <span class="n">Variable</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)))</span>
        <span class="c1"># select the final outputs&#39; result.</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">next_word</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">next_word</span> <span class="o">=</span> <span class="n">next_word</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># concat the most likely word to the result.</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ys</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">next_word</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>
</pre></div>
</div>
<p>Using <a class="reference internal" href="002.html"><span class="doc">beam search</span></a> (as the authors did do), a path is selected by maintaining <em>k</em> beams - i.e. the best-so-far <span class="math notranslate nohighlight">\(k\)</span> options.</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/posts/003.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2018, Charles J. Lovering.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.6.<br/>
    </p>
  </div>
</footer>
  </body>
</html>