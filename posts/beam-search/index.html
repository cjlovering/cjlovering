<!doctype html>
<head>
    <meta charset="utf-8">
    <script src="https://distill.pub/template.v2.js"></script>
    <style>
        circle:hover {
            stroke-width: 2px;
        }
    </style>
    <script src="https://d3js.org/d3.v4.js"></script>
    <script src="graph-viz.js"></script>     
</head>

<d-front-matter>
<script type="text/json">{
    "title": "Beam Search",
    "description": "Exposition of Beam Search.",
    "authors": [
        {
            "author": "--",
            "authorURL": "#",
            "affiliation": "--",
            "affiliationURL": "#"
        }
    ]
}
</script>
</d-front-matter>

<d-title>
    <h1>Beam Search</h1>
    <p>Exposition of Beam Search.</p>
    <div class="l-page" id="vtoc"></div>
</d-title>

<d-byline></d-byline>

<d-article>

<p>Beam search is a method for decoding a sequence given an auto-regressive function that outputs a probability distribution over the next possible symbols. Ideally, a search algorithm would traverse the all paths and select the most probable sequence. However, this is prohibatively expensive.</p>

<p>This search algorithm is often used translation. Beam search is most often used at test time, not during training. For a full implementation see <a href="https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py">OpenNMT</a>. I provide a basic implementation below for reference.</p>

<p>Beam search works iteratively. Naturally, the details depend on the decoding function: a hidden markov model with a memory of 1 consumes the previous symbol; recurrent neural networks are stateful <d-cite key="cho2014learning2"></d-cite>; and transformer networks consume the entire prefix <d-cite key="vaswani2017attention2"></d-cite>.</p>

<h3>Walkthrough</h3>

<p>Given a function which takes a prefix of a sequence and outputs a probability distribution of output symbols for the next item in the sequence, beam-search is an approximate algorithm which searches for the path that results in the most probable sequence. The path with the highest probability to start with, may not end up being the most likely sequence. Log probability is used so that we can sum together the probabilities and avoid floating point errors.</p>

<p>In this example, we will compute symbols until we reach the maximum length of 4, and maintain 2 beams (or hypotheses). There are three output symbols (A, B, C). The log probabilities from the start symbol are -0.39, -0.60, and -0.45.</p>

<figure>
    <div class="l-body">
        <img src="../../_static/images/002/beam-search-table-01.svg">
    </div>
    <figcaption> Step 1.</figcaption>
</figure>

<p>The selected options are those with the highest log probabilities. Now, we will generate the next steps probabilities given these two prefixes (S-A and S-C). Here the search will now continue in different branches. The outputs that are highlighted green indicate that they are the paths with the current highest log probabilities.</p>

<figure>
    <div class="l-body">
        <img src="/_static/images/002/beam-search-table-02.svg">
    </div>
    <figcaption>Step 2.</figcaption>
</figure>

<p>Note that in this time step that one branch will fade completely, as the other branch contains the options with lowest probabilities.</p>

<figure>
    <div class="l-body">
        <img src="/_static/images/002/beam-search-table-03.svg">
    </div>
    <figcaption>Step 3.</figcaption>
</figure>

<p>Finally, beam search will select the path with total lowest log probability.</p>

<figure>
    <div class="l-body">
        <img src="/_static/images/002/beam-search-table-04.svg">
    </div>
    <figcaption>Step 4.</figcaption>
</figure>

<p>Below is a demonstration of how the algorithm searches through the graph, pruning all but the number of parameterized number of beams at each time step.</p>

<div id="target" style="text-align:center;"></div>
<div id="target2" style="text-align:center;"></div>
    
</d-article>

<d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="./refs.bib"></d-bibliography>
</body>


