<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Beam Search &#8212; cjlovering 0.0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/style.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformer Networks" href="003.html" />
    <link rel="prev" title="Byte-Encoding Representation" href="001.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          cjlovering</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="001.html">Byte-Encoding Representation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Beam Search</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="003.html">Transformer Networks</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/research.html">Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profile/professional.html">Professional</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/publications.html">Publications</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Beam Search</a><ul>
<li><a class="reference internal" href="#walkthrough">Walkthrough</a></li>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="beam-search">
<h1>Beam Search<a class="headerlink" href="#beam-search" title="Permalink to this headline">¶</a></h1>
<p>Beam search is a method for decoding a sequence given an auto-regressive function that outputs a probability distribution over the next possible symbols. Ideally, a search algorithm would traverse the all paths and select the most probable sequence. However, this is prohibatively expensive.</p>
<p>This search algorithm is often used translation. Beam search is most often used at test time, not during training. For a full implementation see OpenNMT <a class="footnote-reference" href="#id4" id="id1">[1]</a>. I provide a basic implementation below for reference.</p>
<p>Beam search works iteratively. Naturally, the details depend on the decoding function: a hidden markov model with a memory of 1 consumes the previous symbol; recurrent neural networks are stateful <a class="reference internal" href="#cho2014learning2" id="id2">[1]</a>; and transformer networks consume the entire prefix <a class="reference internal" href="#vaswani2017attention2" id="id3">[2]</a>.</p>
<div class="section" id="walkthrough">
<h2>Walkthrough<a class="headerlink" href="#walkthrough" title="Permalink to this headline">¶</a></h2>
<hr /><p>Given a function which takes a prefix of a sequence and outputs a probability distribution of output symbols for the next item in the sequence, beam-search is an approximate algorithm which searches for the path that results in the most probable sequence. The path with the highest probability to start with, may not end up being the most likely sequence. Log probability is used so that we can sum together the probabilities and avoid floating point errors.</p>
<p>In this example, we will compute symbols until we reach the maximum length of 4, and maintain 2 beams (or hypotheses). There are three possible output symbols (A, B, C). The log probabilities from the start symbol are -0.39, -0.60, and -0.45.</p>
<div class="figure align-center">
<img alt="../_images/beam-search-table-01.svg" src="../_images/beam-search-table-01.svg" width="650px" /></div>
<p>The selected options are those with the highest log probabilities. Now, we will generate the next steps probabilities given these two prefixes (S-A and S-C). Here the search will now continue in different branches. The outputs that are highlighted green indicate that they are the paths with the current highest log probabilities.</p>
<div class="figure align-center">
<img alt="../_images/beam-search-table-02.svg" src="../_images/beam-search-table-02.svg" width="650px" /></div>
<p>Note that in this time step that one branch will fade completely, as the other branch contains the options with lowest probabilities.</p>
<div class="figure align-center">
<img alt="../_images/beam-search-table-03.svg" src="../_images/beam-search-table-03.svg" width="650px" /></div>
<p>Finally, beam search will select the path with total lowest log probability.</p>
<div class="figure align-center">
<img alt="../_images/beam-search-table-04.svg" src="../_images/beam-search-table-04.svg" width="650px" /></div>
<p>Below is a demonstration of how the algorithm searches through the graph, pruning all but the number of parameterized number of beams at each time step.</p>
<html>

<div id="target" style="text-align:center;">
</div>
<div id="target2" style="text-align:center;">
</div>

<style>
circle:hover {
  stroke-width: 2px;
}
</style>

<script src="https://d3js.org/d3.v4.js"></script>
<script>

// Animates the @images a dictionary of file_name to image, which corresponds
// to the list of images @file_names.   
const animate = (images, file_names) => {  
   const WIDTH = 350;

   const buttons = d3.select('#target')
    .append("svg")
    .attr("width", WIDTH)
    .attr("height", 40);
  const canvas = d3.select('#target2')
    .append("svg")
    .attr("width", WIDTH)
    .attr("height", 425);

  // Shows the given image @i, appending it to the canvas.
  const show = i => {
    const img = images[file_names[i]];
    canvas.selectAll('svg').remove();
    canvas.node().appendChild(img);
  }

  // Add buttons which on click will show the corresponding step of the search.
  buttons.selectAll("circle")
    .data([0,1,2,3])
    .enter()
    .append('circle')
    .attr('r', 15)
    .attr('stroke', 'grey')
    .attr('fill', 'white')
    .attr('cx', d => { return d*70 + 65; } )
    .attr('cy', 17)
    .on("mouseover", d => show(d));

  // By default, show the full search graph.
  show(4);
}

// Loads image from the file path, and then saves the image with the 
// given saveImage callback.
const getImage = function(file_path, saveImage) {
  d3.xml(file_path).mimeType("image/svg+xml").get((error, xml) => {
    if (error) throw error;
    const svg = xml.documentElement;
    svg.setAttribute("height", "400px");
    svg.setAttribute("preserveAspectRatio", "xMinYMin meet");
    svg.id = file_path;
    saveImage(file_path, svg);
  });
}


// Load all the images, given @file_names. Store them into a dictionary, 
// file_name to svg object, for future manipulation.
const getImages = function(file_names) {
  let images = {};
  const saveImage = (file_path, img) => {
    images[file_path] = img;
    if (Object.keys(images).length >= file_names.length) {
      animate(images, file_names);
    }
  }
  file_names.forEach(name => getImage(name, saveImage));
  return images;
}

const file_names = [
  '../_static/images/002/beam-search-graph-01.svg',
  '../_static/images/002/beam-search-graph-02.svg',
  '../_static/images/002/beam-search-graph-03.svg',
  '../_static/images/002/beam-search-graph-04.svg',
  '../_static/images/002/beam-search-graph-05.svg'
];
getImages(file_names);

</script>
</html>
</div>
<div class="section" id="code">
<h2>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h2>
<hr /><p>This a reference for how this might look in code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">heapq</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">namedtuple</span>
<span class="n">Beam</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Beam&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ys&#39;</span><span class="p">,</span> <span class="s1">&#39;log_prob_seq&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">beam_search_decode</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">start_symbol</span><span class="p">,</span> <span class="n">beam_count</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Basic beam search decode function. (Untested).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[</span> <span class="n">Beam</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">start_symbol</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="p">]</span>
    <span class="c1"># generate a word up to the max length. the system could represent stop symbols to stop early.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># for each path, operate on that path (autoregressive).</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                <span class="n">memory</span><span class="p">,</span>
                <span class="n">src_mask</span><span class="p">,</span>
                <span class="n">Variable</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">ys</span><span class="p">),</span>
                <span class="n">Variable</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">ys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">candidates</span>
        <span class="p">]</span>
        <span class="c1"># select the most probable words</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="c1"># generate a probability distribution over the possible output symbols.</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="c1"># look at the top beam_count possibilities in each path.</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">beam_count</span><span class="p">):</span>
                <span class="n">prob</span><span class="p">,</span> <span class="n">next_word</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
                <span class="n">next_word</span> <span class="o">=</span> <span class="n">next_word</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">dist</span><span class="p">[</span><span class="n">next_word</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span> <span class="c1"># zero out.</span>
                <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">ys</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">next_word</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">log_prob_seq</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">log_prob_seq</span> <span class="o">+</span> <span class="n">log_prob</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Beam</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">log_prob_seq</span><span class="p">))</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="n">beam_count</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">b</span><span class="o">.</span><span class="n">log_prob_seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>
</pre></div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<hr /><p>It is possible that if we searched all the paths, extending out from options we pruned early on by keeping only the top beams, we may have recieved a different result. If one used depth-first search, each path would be evaluated, but at exponential cost. For example, with a vocabulary of 3 possible output symbols, and a maximum length of 4 symbols, finding the most likely path would cost 81 operations, compared to the 8 we demonstrate above. Beam search does not maintain all possibilities. It keeps the current best-so-far h possibilties, only requiring a linear number of operations.</p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td><a class="reference external" href="https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py">https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py</a></td></tr>
</tbody>
</table>
<p class="rubric">References</p>
<p id="bibtex-bibliography-posts/002-0"><table class="docutils citation" frame="void" id="cho2014learning2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>Kyunghyun Cho, Bart Van&nbsp;Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. <em>arXiv preprint arXiv:1406.1078</em>, 2014. URL: <a class="reference external" href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="vaswani2017attention2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[2]</a></td><td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Advances in Neural Information Processing Systems</em>, 5998–6008. 2017. URL: <a class="reference external" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a>.</td></tr>
</tbody>
</table>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/posts/002.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2018, Charles J. Lovering.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.7.<br/>
    </p>
  </div>
</footer>
  </body>
</html>