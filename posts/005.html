<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Decomposable Attention &#8212; cjlovering 0.0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://d3js.org/d3.v5.min.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          cjlovering</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="001.html">Byte-Encoding Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="002.html">Beam Search</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="003.html">Transformer Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="004.html">Neural Turing Machines</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/research.html">Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profile/professional.html">Professional</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profile/publications.html">Publications</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Decomposable Attention</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="decomposable-attention">
<h1>Decomposable Attention<a class="headerlink" href="#decomposable-attention" title="Permalink to this headline">¶</a></h1>
<h4>A Decomposable Attention Model for Natural Language Inference</h4>
<hr />
<table>
  <thead>
    <tr>
      <th width="150px"> Authors </th> 
      <th width="150px"> Affliations </th> 
      <th width="300px"> Links </th> </tr>
  </thead>
  <tbody>
      <td>Ankur P. Parikh</td>
      <td>Google</td>
      <td><a href="https://arxiv.org/pdf/1606.01933.pdf"> PDF </a> </td>
    </tr>
    <tr>
      <td>Oscar Tackstrom</td>
      <td>Google</td>
    </tr>
    <tr>
      <td>Dipanjan Das</td>
      <td>Google</td>
    </tr>
    <tr>
      <td>Jakob Uszkoreit</td>
      <td>Google</td>
    </tr>
  </tbody>
</table>
<hr /><p>Decomposable Attention is a SOTA model for textual entailment [parikh2016decomposable].</p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<hr /><p>Textual entailment is the task of determining if one can infer a sentence from another sentence. Its been lauded as an important task to Natural Language Understanding (NLU), as it is hypothesized that only a model capable of understanding the semantics of the sentences is able to perform entailment. This calls to mind the supposition of XYZ [XYZ] that to understand a sentence is to understand its truth conditions. Thus, textual entailment becomes an application of this restriction.</p>
<p>However, the authors of Decomposable Attention show that, at least on existing datasets, it is not necessary to build sophisticated sentence representations, but instead believe that comparing sub-parts of sentences is suffient for textual entailment. They then propose that all we must do is appropriately align the relevant subparts of the sentences.</p>
</div>
<div class="section" id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h2>
<hr /><p>The authors used the proposed Neural Turing Machine (NTM) to solve a range of basic data manipulation tasks. An external controller network utilizes NTM’s API of blurry read and write operations to tackle the task.</p>
<div class="figure align-center">
<img alt="../_images/simple-architecture1.svg" height="150px" src="../_images/simple-architecture1.svg" /></div>
<p>The controller has a number of heads which each other read or write from the memory. The number and purpose of the heads is static (part of the network configuration). The generic API for the controller as a whole is only a sequence of vectors (describing a problem) and then the controller will output an answer for the given problem once a termination symbol is reached. There is no intermittent guidance on how the controller uses the NTM to organize information. The authors hypothesized, and at least to some degree demonstrated, that the controller and the NTM in concert learn to construct generalizable programs.</p>
<p>The read operation is a weighted sum of the stateful memory <span class="math notranslate nohighlight">\(M \in \mathbb{R}^{N \times M}\)</span> as determined by a mask <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{N}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\sum_i M_i w_i \in \mathbb{R}^{M}\]</div>
<p>The write is more complicated; it uses something akin to the gating mechanism in an LSTM to update memory positions. The update is applied across all of its memory, but again, the update is controlled using the same mask as above.</p>
<p>TODO: Add the writing section, add diagrams for reading and writing.</p>
<p>The NTM uses a novel mechanism for addressing content inside its memory.</p>
<div class="figure align-center" id="id3">
<img alt="../_images/addressing.svg" height="450px" src="../_images/addressing.svg" /><p class="caption"><span class="caption-text">Addressing mechanism to create a mask used for reading and writing.</span></p>
</div>
<p>TODO: Detail specific layers.</p>
<p>TODO: Provide basic code that shows how this code work.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-posts/005-0"><table class="docutils citation" frame="void" id="graves2014neural" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. <em>arXiv preprint arXiv:1410.5401</em>, 2014.</td></tr>
</tbody>
</table>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/posts/005.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2018, Charles J. Lovering.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>