{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCxUhB3lr0Ox"
   },
   "source": [
    "#  Attention Is All You **Need**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVu2qNbDr3Uz"
   },
   "source": [
    "This notebook, hosted on [colab](http://colab), *hopefully* explains some of the details from this [paper](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "Most of the paper and its techniques are clear, but for myself, exactly how it was able to operate on sequences of unknown length was not clear. I will investigate this, and otherwise highlight some of the techniques and concepts used by their proposed *Transformer Networks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cxm9VRjRqcKE"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision\n",
    "!pip install -q sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "feK7aGKVqr3M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlUGF_rCspfv"
   },
   "source": [
    "## **Overview**\n",
    "The general encoder decoder framework is used: an input sequence of symbols, $x = { x_1, x_2, \\dots, x_n }$, is encoded into a sequence of continuous variables,  $z = { z_1, z_2, \\dots, z_n }$. This is then decoded into a sequence of symbols, $y = { y_1, y_2, \\dots, y_n }$.\n",
    "\n",
    "## Input Representation\n",
    "\n",
    "This work used a Byte Pair Encoding scheme. This is a subword tokenization of your vocabulary. An example of this from Sennrich et al. is as follows:\n",
    "\n",
    "> vocab = ‘low’, ‘lowest’, ‘newer’, ‘wider’\n",
    "\n",
    "> OOV = 'lower' --> 'low_', 'er_'\n",
    "\n",
    "This is much more valuable than a UNK symbol. To build this representation, an iterative algorithm can be used to link together the most common segments, starting with character pairs. Below is the psedudo code provided by the original authors with slight annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1533083119133,
     "user": {
      "displayName": "Charles Lovering",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106347288410532794654"
     },
     "user_tz": 420
    },
    "id": "N8FthBCfnMEs",
    "outputId": "66360749-7aa1-4ab8-da5a-9cb07314d55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'r')\n",
      "('er', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('low', '</w>')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"For the given merge step, get the bigram counts, for every pair of symbols (strings).\n",
    "\n",
    "    For each word and its freq in the corpus, add the number of instances for each of its subword parts.\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "        pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram_pattern = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram_pattern + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {\n",
    "  'l o w </w>' : 5, \n",
    "  'f a r t h e s t </w>' : 5,\n",
    "  'n e w e r </w>': 5,\n",
    "  'w i d e r </w>': 5 }\n",
    "num_merges = 5\n",
    "transforms = set()\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "\n",
    "    try:\n",
    "        best = max(pairs, key=pairs.get)\n",
    "    except ValueError:\n",
    "        break\n",
    "    if pairs[best] < 2:\n",
    "        print('no pair has frequency > 1. Stopping\\n')\n",
    "        break\n",
    "    transforms.add(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HpAAVwAsB_A"
   },
   "source": [
    "## References\n",
    "\n",
    "### Papers\n",
    "\n",
    "Vaswani. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "Sennrich. [BPE w/ Rare Words](http://www.aclweb.org/anthology/P16-1162)\n",
    "\n",
    "### Notebooks\n",
    "\n",
    "### Github\n",
    "\n",
    "[Byte Pair Enconding](https://github.com/bheinzerling/bpemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_eib-_wWsDBV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Attention Is All You Need.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
