Byte-Encoding Representation
----------------------------

.. raw:: html
  :file: assets/header/001.html

This is a subword tokenization of a vocabulary. An example of how this would work and be useful at test time from Sennrich et al. :cite:`sennrich2015neural2` is as follows:

.. code-block:: python3

  V = { "low", "lowest", "newer", "wider" }
  BCE("lower")
  > "low", "er"

This is much more valuable than a symbol for an unknown word (UNK). Using the two subword segments, it would not be surpirising if a model would be able to intepret "lower" correctly.

To build this representation, an iterative algorithm can be used to link together the most common segments, starting with character pairs. Below is the pseudo code provided by the original authors with a few changes. This algorithm is iterative. The author provides a more optimized implementation that batches and avoids recomputation. It starts bottom up, getting the bigram counts for every pair of symbols. These symbols will startout as characters but in later steps be common strings.

.. literalinclude:: assets/code/001/bce.py
   :lines: 4-14

Next, given the bi-counts of a vocabulary, merge the vocabulary to remove repetitions of this bigram.

.. literalinclude:: assets/code/001/bce.py
   :lines: 16-

.. code-block:: python3

    >>> vocab = {
        'l o w </w>' : 5, 
        'f a r t h e s t </w>' : 5,
        'n e w e r </w>': 5,
        'w i d e r </w>': 5 }
    5
    >>> byte_pair_encoding(vocab)
    {'l o w </w>': 5, 'f a r t h e s t </w>': 5, 'n e w er </w>': 5, 'w i d er </w>': 5}
    {'l o w </w>': 5, 'f a r t h e s t </w>': 5, 'n e w er</w>': 5, 'w i d er</w>': 5}
    {'lo w </w>': 5, 'f a r t h e s t </w>': 5, 'n e w er</w>': 5, 'w i d er</w>': 5}
    {'low </w>': 5, 'f a r t h e s t </w>': 5, 'n e w er</w>': 5, 'w i d er</w>': 5}
    {'low</w>': 5, 'f a r t h e s t </w>': 5, 'n e w er</w>': 5, 'w i d er</w>': 5}

.. rubric:: References

.. bibliography:: assets/refs/refs_001.bib
  :style: unsrt