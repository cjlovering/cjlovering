<head>
    <link rel="shortcut icon" href="_static/favicon.ico">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
        integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
        crossorigin="anonymous"></script>
</head>

<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v2.js"></script>

<body>
    <d-front-matter>
        <script type="text/json">{
        "title": "Charles Lovering",
        "description": "--",
        "authors": [
            {
                "author": "Charlie Lovering",
                "authorURL": "https://github.com/cjlovering",
                "affiliation": "Brown University",
                "affiliationURL": "https://cs.brown.edu/people/epavlick/students.html",
                "published": "2020"
            }
        ]
    }
    </script>
    </d-front-matter>

    <d-title>
        <h1>Charles Lovering</h1>
        <p> Computer Science PhD Student (NLU).</p>
        <div class="l-page" id="vtoc"></div>
    </d-title>

    <d-article class="centered">
        <h2>Hello</h2>
        <p>Interesting stuff below &darr;; a bit about me <a href="./profile/index.html">here</a> &larr;. </p>
        <div style="max-width:600px;display:grid;grid-template-columns:100%;grid-column-gap:0%">
            <h2>Projects</h2>
            <h3>Recent Work</h3>
            <ol>
                <li><a href="https://openreview.net/forum?id=mNtmhaDkAr">Charles Lovering, Rohan Jha, Tal Linzen, Ellie
                        Pavlick. Information-theoretic Probing Explains Reliance on Spurious Features.
                        ICLR, 2021.</a></li>
                <li><a href="https://arxiv.org/pdf/2004.15012.pdf">Rohan Jha, Charles Lovering, Ellie Pavlick. Does Data
                        Augmentation Improve Generalization in NLP?
                        2020. PREPRINT.</a></li>
                <li><a href="https://arxiv.org/pdf/2010.04872.pdf">Charles Lovering, Ellie Pavlick. Self-play for Data
                        Efficient Language Acquisition. 2020. PREPRINT.
                    </a></li>
            </ol>
            <h3>Fun Visualizations</h3>
            <ul>
                <li><a href="https://observablehq.com/collection/@xenocidist/lindenmayer-systems">Ferns:</a>
                    Visualizations of Lindenmayer systems, generating beautiful ferns!
                </li>
                <li><a href="https://cjlovering.github.io/playground/">Playground:</a> Different interactive
                    visualizations, including a hexagon-based game-of-life, random gradient walks, and (my favorite)
                    bouncing balls.</li>
            </ul>
            <h3>Unpublished Work</h3>
            <ul>
                <li><a href="posts/interpretable-reinforcement-learning/index.html">Interpretable Reinforcement
                        Learning: </a> Reimplementation of attention-based reinforcement learning.</li>
                <li><a href="posts/listicles/index.html">Listicles:</a> A dataset drawn extracted from online listicles.
                </li>
            </ul>
        </div>

        <div style="max-width:550px;display:grid;grid-template-columns:100%;grid-column-gap:0%">
            <h2>Notes</h2>
            <div>
                <h3>Talks (slides)</h3>
                <ul>
                    <li><a href="https://openreview.net/forum?id=mNtmhaDkAr">Information-theoretic Probing Explains
                            Reliance on Spurious Features.</a> @ NLP & Fairness, Interpretability, and
                        Robustness; Google. Fall, 2020.</li>
                    <li><a href=./presentations/mdl.pdf>Minimum Description Length Probing</a> @ Language Understanding
                        and
                        Representations; Brown University. Summer, 2020.</li>
                    <li><a href=./presentations/transformer-tutorial.pdf>Transformers</a> @ Language Understand and
                        Representations; Brown University. Summer, 2019.</li>
                </ul>
            </div>
            <div>
                <h3>Methods (expositions)</h3>
                <ul>
                    <li><a href="posts/byte-encoding/index.html">Byte Encoding Representation</a></li>
                    <li><a href="posts/beam-search/index.html">Beam Search</a></li>
                </ul>
            </div>
            <div>
                <h3>Models (expositions)</h3>
                <ul>
                    <li><a href="posts/transformer-networks/index.html">Transformer Networks</a></li>
                    <li><a href="posts/neural-turing/index.html">Neural Turing Machine</a></li>
                </ul>
            </div>
        </div>
        <div style="max-width:550px;display:grid;grid-template-columns:100%;grid-column-gap:0%">
            <h2>Etc</h2>
            <div>
                <ul>
                    <li><a href=./posts/code-tricks/index.html>Code Tricks</a></li>
                </ul>
            </div>
        </div>
    </d-article>

    <d-appendix>
        <h3 id="acknowledgments">Acknowledgments</h3>
        <p>This site replicates the <a href="https://distill.pub/journal/">distill</a> design.</p>
        <p>Adobe XD CC for diagrams, D3 for visualizations, and PyTorch for implementations (a plurality of the time).
        </p>
        <p>If a diagram (or notes or presentation) proves useful, let me know (or cite this website)! </p>
    </d-appendix>
</body>